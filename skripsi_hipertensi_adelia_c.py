# -*- coding: utf-8 -*-
"""Skripsi Hipertensi Adelia C.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13I5L19nIzGZHAfrCZWHgsaJy8P99imDB

# **Import Library**
"""

import pandas as pd

"""# **Import Data**"""

# Input data
from google.colab import drive
drive.mount('/content/drive')
data=pd.read_excel('/content/drive/MyDrive/Skripsi/Hipertensi.xlsx')

"""# **Data Understanding**"""

# Menampilkan data
print(data)

# Informasi jumlah baris, kolom, dan struktur data
print("Jumlah baris dan kolom:", data.shape)
data.info()

# Variabel independen dan dependen
x = data.drop(['Status Hipertensi'], axis=1)
y = data['Status Hipertensi']

# Variabel independen
x = data.drop(['Status Hipertensi'], axis=1)
print(x)

# Variabel dependen
y = data['Status Hipertensi']
print(y)

"""# **Analisis Deskriptif**"""

# Deskripsi statistik variabel independen numerik
print(data.describe())

# Deskripsi statistik variabel independen kategorik
columns_to_analyze = [
    'Jenis Kelamin', 'Risiko Merokok', 'Kurang Aktivitas Fisik',
    'Gula Berlebihan', 'Garam Berlebihan', 'Lemak Berlebihan',
    'Kurang Makan Buah dan Sayur', 'Konsumsi Alkohol', 'Status Hipertensi'
]

for column in columns_to_analyze:
    counts = data[column].value_counts()
    percentage = counts / counts.sum() * 100
    print(f"\nFrekuensi dan Persentase untuk {column}:")
    print(pd.DataFrame({'Frekuensi': counts, 'Persentase (%)': percentage}))

# # Deskripsi statistik variabel dependen
status_counts = y.value_counts()
status_percentage = status_counts / len(y) * 100
print("\nFrekuensi dan Persentase Status Hipertensi:")
print(pd.DataFrame({'Frekuensi': status_counts, 'Persentase (%)': status_percentage}))

"""# **Data Preparation**

## **Missing Value**
"""

# Mengecek nilai yang hilang
print("Jumlah Missing Values per Kolom:")
print(data.isnull().sum())

# Menampilkan total nilai NaN
total_nan = data.isnull().sum().sum()
print("Total nilai NaN di dataset:", total_nan)

# Menampilkan baris dengan nilai hilang
missing_data = data[data.isnull().any(axis=1)]
print("Jumlah baris dengan Missing Values:", len(missing_data))
print(missing_data)

# Menghapus baris dengan Missing Values
data.dropna(inplace=True)

# Verifikasi setelah penanganan
print("Jumlah baris setelah Missing Values dihapus:", len(data))
print(data.isnull().sum())

"""## **Duplikasi Data**"""

# Mengecek dan menampilkan baris duplikat
duplicate_count = data.duplicated().sum()
print(f"Jumlah baris duplikat: {duplicate_count}")

if duplicate_count > 0:
    duplicates = data[data.duplicated()]
    print("Baris yang duplikat:")
    print(duplicates)

# Menghapus baris duplikat
data.drop_duplicates(inplace=True)

# Verifikasi setelah penghapusan
print("Jumlah baris setelah duplikat dihapus:", len(data))

"""## **Imbalance**"""

# Display the counts of each class in the 'Status Hipertensi' column before oversampling
print("Jumlah data:")
print(data['Status Hipertensi'].value_counts())

"""## **Feature Encoding**"""

# Mapping encoding untuk variabel kategorik
encoding_dict = {
    'Jenis Kelamin': {'LAKI-LAKI': 0, 'PEREMPUAN': 1},
    'Risiko Merokok': {'TIDAK': 0, 'YA': 1},
    'Kurang Aktivitas Fisik': {'TIDAK': 0, 'YA': 1},
    'Gula Berlebihan': {'TIDAK': 0, 'YA': 1},
    'Garam Berlebihan': {'TIDAK': 0, 'YA': 1},
    'Lemak Berlebihan': {'TIDAK': 0, 'YA': 1},
    'Kurang Makan Buah dan Sayur': {'TIDAK': 0, 'YA': 1},
    'Konsumsi Alkohol': {'TIDAK': 0, 'YA': 1},
    'Status Hipertensi': {'NORMAL': 0, 'HIPERTENSI': 1}
}

# Melakukan encoding
data.replace(encoding_dict, inplace=True)

# Menghapus kolom yang tidak diperlukan
data.drop(columns=['Nama Pasien'], inplace=True, errors='ignore')

print("\nData setelah Transformasi:")
print(data)

# Menyimpan data yang telah ditransformasi ke Google Drive
file_path = '/content/drive/MyDrive/Skripsi/Data Hasil Transformasi.csv'
data.to_csv(file_path, index=False)

print(f"\nData telah disimpan ke Google Drive dengan nama file: {file_path}")

# Menampilkan 30 data terdepan
print("\n30 Data Terdepan:")
print(data.head(30))

# Menampilkan 5 data terakhir
print("\n5 Data Terbelakang:")
print(data.tail(5))

"""# **Metode**"""

# Import Library
import pandas as pd
import numpy as np
from google.colab import drive
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

# Menampilkan data
print(data)

# Menampilkan informasi data
data.info()

# Variabel independen
x = data.drop(['Status Hipertensi'], axis=1)
print(x)

# Variabel dependen
y = data['Status Hipertensi']
print(y)

"""### **Split Data**"""

from sklearn.model_selection import train_test_split

# Pembagian training dan testing set
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.1, random_state=0
)

# Menampilkan jumlah data
print("Jumlah data sebelum dibagi:")
print("Total data:", len(data))
print("Jumlah data training:", len(x_train))
print("Jumlah data testing:", len(x_test))

# Menampilkan data training dan testing
print("\nData Training (x_train):")
print(x_train.head())  # Menampilkan 5 baris pertama dari data training

print("\nData Testing (x_test):")
print(x_test.head())   # Menampilkan 5 baris pertama dari data testing

print("\nTarget Training (y_train):")
print(y_train.head())  # Menampilkan 5 baris pertama dari target training

print("\nTarget Testing (y_test):")
print(y_test.head())   # Menampilkan 5 baris pertama dari target testing

# Menggabungkan Fitur dan Target dalam Data Training dan Testing
data_train = pd.concat([x_train, y_train], axis=1)
data_test = pd.concat([x_test, y_test], axis=1)

# Menyimpan Data Training dan Testing ke Google Drive
data_train.to_csv('/content/drive/MyDrive/Skripsi/Aslii Train.csv', index=False)
data_test.to_csv('/content/drive/MyDrive/Skripsi/Aslii Test.csv', index=False)

# Menampilkan jumlah data train dan test
print("Jumlah data training:", data_train.shape[0])
print("Jumlah data testing:", data_test.shape[0])

print("Data training dan testing telah disimpan ke Google Drive.")

# Menampilkan 10 data terdepan dan 2 data terakhir untuk data training
print("\n10 Data terdepan - Data Training:")
print(data_train.head(10))

print("\n2 Data terakhir - Data Training:")
print(data_train.tail(2))

# Menampilkan 30 data terdepan dan 2 data terakhir untuk data testing
print("\n10 Data terdepan - Data Testing:")
print(data_test.head(10))

print("\n2 Data terakhir - Data Testing:")
print(data_test.tail(2))

"""## **Naive Bayes Classifier**"""

# Import library
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

# Menampilkan data
print(data)

# Menampilkan informasi data
data.info()

# Variabel independen
x = data.drop(['Status Hipertensi'], axis=1)
print(x)

# Variabel dependen
y = data['Status Hipertensi']
print(y)

# Pembagian training dan testing set
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.1, random_state=0
)

# Membuat model Naive Bayes
model_nb = GaussianNB()
model_nb.fit(x_train, y_train)

# Prediksi menggunakan data test
y_pred = model_nb.predict(x_test)

# Menampilkan hasil prediksi
print("Hasil Prediksi:")
print(y_pred)

np.array(y_test)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("\nConfusion Matrix:")
print(cm)

# Menghitung Akurasi
accuracy = accuracy_score(y_test, y_pred)
print("\nAkurasi:")
print(accuracy)

# Classification Report
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

from sklearn.metrics import classification_report

# Classification Report dengan 3 digit
print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=3))

"""## **KNN**"""

# Import Library
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from google.colab import drive
from sklearn import preprocessing
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import cross_val_score, KFold, cross_validate
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, accuracy_score, classification_report

# Menampilkan data
print (data)

# Menampilkan informasi data
data.info()

# Variabel independen
x = data.drop(['Status Hipertensi'], axis=1)
print(x)

# Variabel dependen
y = data['Status Hipertensi']
print(y)

##### INI SEKKKKKKKKKKKKK

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import pandas as pd

# Memisahkan variabel numerik dan kategorik, dengan menghapus 'Status Hipertensi' dari kategori
numerical_columns = ['Berat Badan', 'Tinggi Badan', 'Tekanan Darah Sistolik', 'Tekanan Darah Diastolik', 'Lingkar Perut']
categorical_columns = ['Jenis Kelamin', 'Risiko Merokok', 'Kurang Aktivitas Fisik', 'Gula Berlebihan',
                       'Garam Berlebihan', 'Lemak Berlebihan', 'Kurang Makan Buah dan Sayur',
                       'Konsumsi Alkohol']  # 'Status Hipertensi' dihapus karena ini adalah variabel Y

# Min-Max Scaling untuk seluruh data
scaler = MinMaxScaler()

# Terapkan pada seluruh data, baik numerik maupun kategorik
x_numerical = x[numerical_columns]
x_categorical = x[categorical_columns]

# Standarisasi data numerik
x_numerical_scaled = scaler.fit_transform(x_numerical)

# Menggabungkan data numerik yang sudah distandarisasi dengan data kategorik
x_final = pd.DataFrame(x_numerical_scaled, columns=numerical_columns)
x_final = pd.concat([x_final, x_categorical.reset_index(drop=True)], axis=1)

# Menyimpan data hasil standarisasi secara lengkap
full_data_path = '/content/drive/MyDrive/Skripsi/Full_Standardized_Data.csv'
x_final.to_csv(full_data_path, index=False)
print(f"Data yang sudah distandarisasi secara lengkap telah disimpan ke Google Drive pada {full_data_path}.")

# Pembagian training dan testing set (sekarang data x_final yang sudah distandarisasi)
x_train, x_test, y_train, y_test = train_test_split(x_final, y, test_size=0.1, random_state=0)

# Menyimpan data training dan testing yang sudah distandarisasi ke Google Drive
train_path = '/content/drive/MyDrive/Skripsi/Standardizedd_Train.csv'
test_path = '/content/drive/MyDrive/Skripsi/Standardizedd_Test.csv'

data_train_standardized = pd.concat([x_train, y_train.reset_index(drop=True)], axis=1)
data_test_standardized = pd.concat([x_test, y_test.reset_index(drop=True)], axis=1)

data_train_standardized.to_csv(train_path, index=False)
data_test_standardized.to_csv(test_path, index=False)

print(f"Data training dan testing yang telah distandarisasi telah disimpan ke Google Drive pada {train_path} dan {test_path}.")

# Pembagian training dan testing set
x_train, x_test, y_train, y_test = train_test_split(
    x, y, test_size=0.1, random_state=0
)

# 10-Fold Cross Validation
CV = KFold(n_splits=10, shuffle=False)

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.neighbors import KNeighborsClassifier
import pandas as pd

# Memisahkan variabel numerik dan kategorik
numerical_columns = ['Berat Badan', 'Tinggi Badan', 'Tekanan Darah Sistolik', 'Tekanan Darah Diastolik', 'Lingkar Perut']
categorical_columns = ['Jenis Kelamin', 'Risiko Merokok', 'Kurang Aktivitas Fisik', 'Gula Berlebihan',
                       'Garam Berlebihan', 'Lemak Berlebihan', 'Kurang Makan Buah dan Sayur',
                       'Konsumsi Alkohol']

# Menggunakan MinMaxScaler untuk variabel numerik
scaler = MinMaxScaler()
x_train_numerical = x_train[numerical_columns]
x_test_numerical = x_test[numerical_columns]

x_train_numerical = scaler.fit_transform(x_train_numerical)
x_test_numerical = scaler.transform(x_test_numerical)

# Menyimpan kembali kolom kategorik tanpa distandarisasi
x_train_categorical = x_train[categorical_columns]
x_test_categorical = x_test[categorical_columns]

# Menggabungkan kembali data numerik yang sudah distandarisasi dengan data kategorik
x_train_final = pd.DataFrame(x_train_numerical, columns=numerical_columns)
x_test_final = pd.DataFrame(x_test_numerical, columns=numerical_columns)

x_train_final = pd.concat([x_train_final, x_train_categorical.reset_index(drop=True)], axis=1)
x_test_final = pd.concat([x_test_final, x_test_categorical.reset_index(drop=True)], axis=1)

# Cek hasil
print(x_train_final.head())
print(x_test_final.head())

# Pilih parameter k antara 1 sampai 30
k_values = range(1, 31)
k_scores = []

# Cross-validation untuk setiap nilai k
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, x_train_final, y_train, cv=10, scoring='accuracy')  # Gunakan x_train_final
    k_scores.append(scores.mean())

# Menampilkan nilai k dengan skor terbaik
optimal_k = k_values[k_scores.index(max(k_scores))]
print(f"Nilai k optimal: {optimal_k} dengan akurasi rata-rata: {max(k_scores):.4f}")

# Pilih parameter k antara 1 sampai 30
k_values = range(1, 31)
k_scores = []

# Cross-validation untuk setiap nilai k
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    scores = cross_val_score(knn, x_train_final, y_train, cv=10, scoring='accuracy')  # Gunakan x_train_final
    k_scores.append(scores.mean())

# Plot
plt.plot(k_values, k_scores, marker='o')

plt.xlabel('k Values')
plt.ylabel('Cross-Validated Accuracy')
plt.title('K-NN Varying number of neighbors')
plt.grid()
plt.show()

print(k_scores)

# Menampilkan hasil cross-validated accuracy untuk setiap nilai k
for k, score in zip(k_values, k_scores):
    print(f"k = {k} : Cross-Validated Accuracy = {score:.4f}")

# Memilih parameter k terbaik
best_index = np.argmax(k_scores)
best_k_value = k_values[best_index]

print("Nilai k terbaik:", best_k_value)

# Klasifikasi k-NN
knn = KNeighborsClassifier(n_neighbors=best_k_value)  # Pastikan best_k_value sudah didefinisikan
knn.fit(x_train_final, y_train)  # Gunakan x_train_final yang sudah distandarisasi dan digabungkan
y_pred = knn.predict(x_test_final)  # Gunakan x_test_final yang sudah distandarisasi dan digabungkan

# Menampilkan hasil prediksi
print(y_pred)

np.array(y_test)

# Menghitung Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Menghitung akurasi
accuracy = accuracy_score(y_test, y_pred)
print("Akurasi:", accuracy)

# Menampilkan Classification Report
print("Classification Report:")
print(classification_report(y_test, y_pred))

from sklearn.metrics import classification_report

# Classification Report dengan 3 digit
print("\nClassification Report:")
print(classification_report(y_test, y_pred, digits=3))

# Simpan model k-NN ke file
import pickle
knn_model_path = '/content/drive/MyDrive/Skripsi/knn_model.pkl'
with open(knn_model_path, 'wb') as file:
    pickle.dump(knn, file)
print(f"Model k-NN telah disimpan ke {knn_model_path}")